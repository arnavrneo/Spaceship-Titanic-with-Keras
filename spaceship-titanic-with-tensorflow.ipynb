{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align='center'> Spaceship Titanic with Tensorflow </h1>","metadata":{}},{"cell_type":"markdown","source":"<center><img src='https://pyimagesearch.com/wp-content/uploads/2019/10/keras_vs_tfdotkeras_header.png'></img></center>","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Thank you for reading this notebook!</p>\n\n<p style='font-size: 17px'>This notebook aims to solve the <strong>Spaceship Titanic</strong> mystery with the help of <strong>Neural Networks</strong> using <strong>Tensorflow-keras.</strong></p>\n    \n<p style='font-size: 17px'>For detailed EDA with LightGBM, check out this notebook: <a style='font-weight: 900'>https://www.kaggle.com/code/arnavr10880/spaceshiptitanic-eda-fe-optimization</a></p>\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"# Notebook contents\n\n- <a href='#libraries'> Required Libraries </a>\n- <a href='#dataset'> Reading the dataset </a>\n- <a href='#pre'> Pre-processing the data </a>\n- <a href='#engg'> Feature Engineering </a>\n- <a href='#prepare'> Preparing for training </a>\n- <a href='#nn'> Neural Network Architecture (Model) </a>\n- <a href='#tuning'> Model Training (with hyperparameter tuning) </a>\n- <a href='#viz'> Visualizing evaluation </a>\n    - <a href='#accuracy'> Plotting Accuracy & Loss Plots </a>\n    - <a href='#confusion'> Confusion Matrix </a>\n- <a href='#final'> Performing final predictions </a>\n\n<br/><br/>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<a id='libraries'></a>\n# Required Libraries","metadata":{}},{"cell_type":"code","source":"# !pip install keras-tuner --upgrade","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keras & Tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nimport keras_tuner as kt\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# sklearn libraries\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.preprocessing import LabelEncoder, Normalizer","metadata":{"execution":{"iopub.status.busy":"2022-04-21T03:44:09.759900Z","iopub.execute_input":"2022-04-21T03:44:09.760222Z","iopub.status.idle":"2022-04-21T03:44:18.018249Z","shell.execute_reply.started":"2022-04-21T03:44:09.760131Z","shell.execute_reply":"2022-04-21T03:44:18.017221Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"<a id='dataset'></a>\n# Reading the dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/spaceship-titanic/train.csv')\ntest = pd.read_csv('../input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-21T03:47:41.491018Z","iopub.execute_input":"2022-04-21T03:47:41.491710Z","iopub.status.idle":"2022-04-21T03:47:41.593854Z","shell.execute_reply.started":"2022-04-21T03:47:41.491656Z","shell.execute_reply":"2022-04-21T03:47:41.591482Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<a id='pre'></a>\n# Pre-processing the data","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Here, we'll do some pre-processing which involves filling the missing values.</p>","metadata":{}},{"cell_type":"code","source":"imputer_cols = [\"Age\", \"FoodCourt\", \"ShoppingMall\", \"Spa\", \"VRDeck\" ,\"RoomService\"]\nimputer = SimpleImputer()\nimputer.fit(train[imputer_cols])\ntrain[imputer_cols] = imputer.transform(train[imputer_cols])\ntest[imputer_cols] = imputer.transform(test[imputer_cols])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['HomePlanet'].fillna('Earth', inplace=True)\ntrain['CryoSleep'].fillna('False', inplace=True)\ntrain['Cabin'].fillna('None', inplace=True)\ntrain['Destination'].fillna('TRAPPIST-1e', inplace=True)\ntrain['VIP'].fillna('False', inplace=True)\n\ntest['HomePlanet'].fillna('Earth', inplace=True)\ntest['CryoSleep'].fillna('False', inplace=True)\ntest['Cabin'].fillna('G/109/P', inplace=True)\ntest['Destination'].fillna('TRAPPIST-1e', inplace=True)\ntest['VIP'].fillna('False', inplace=True)\n\ntrain['Name'].fillna('no name', inplace=True)\ntest['Name'].fillna('no name', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='engg'></a>\n# Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>We'll be doing some feature engineering by creating new features out of the others.</p>","metadata":{}},{"cell_type":"code","source":"train['Deck'] = train['Cabin'].apply(lambda x:str(x)[:1])\ntrain['Side'] = train['Cabin'].apply(lambda x:str(x)[-1:])\ntest['Deck'] = train['Cabin'].apply(lambda x:str(x)[:1])\ntest['Side'] = train['Cabin'].apply(lambda x:str(x)[-1:])\n\ntrain['PGroup'] = train['PassengerId'].apply(lambda x: x.split('_')[0])\ntrain['PNumber'] = train['PassengerId'].apply(lambda x: x.split('_')[1])\ntest['PGroup'] = test['PassengerId'].apply(lambda x: x.split('_')[0])\ntest['PNumber'] = test['PassengerId'].apply(lambda x: x.split('_')[1])\n\ntrain['LastName'] = train['Name'].apply(lambda x:x.split(\" \")[1])\ntest['LastName'] = train['Name'].apply(lambda x:x.split(\" \")[1])\n\ntrain['AllServices'] = train['RoomService'] + train['FoodCourt'] + train['ShoppingMall'] + train['Spa'] + train['VRDeck']\ntest['AllServices'] = test['RoomService'] + test['FoodCourt'] + test['ShoppingMall'] + test['Spa'] + test['VRDeck']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>As machine learning models only love numbers, we'll encode the categorical columns using <strong>Label Encoder</strong>.</p>","metadata":{}},{"cell_type":"code","source":"label_cols = [\"HomePlanet\", \"CryoSleep\", \"Destination\" ,\"VIP\", \"Deck\", \"Side\", \"LastName\", \"PGroup\", \"PNumber\"]\n\nfor i in label_cols:\n    train[i] = train[i].astype(str)\n    test[i] = test[i].astype(str)\n    train[i] = LabelEncoder().fit_transform(train[i])\n    test[i] = LabelEncoder().fit_transform(test[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['Transported'] = LabelEncoder().fit_transform(train['Transported'])\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='prepare'></a>\n# Preparing for training","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>We'll drop following columns as they are no longer needed now.</p>","metadata":{}},{"cell_type":"code","source":"cols_to_drop = ['Name', 'PassengerId', 'Cabin']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['Transported']\ntrain.drop(['Name', 'PassengerId', 'Cabin', 'Transported'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p class=\"alert alert-block alert-info\" style='font-size: 17px'> ðŸ“Œ <strong>Normalizing</strong> or scaling is one of the most important step in any machine learning process. It helps the values to get scaled at the same level for better prediction efficiency. Here, we'll use Normalizer to normalize the data. </p>","metadata":{}},{"cell_type":"code","source":"transformer = Normalizer().fit(train) \ntransformer\ntrain=transformer.transform(train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Then, we can split the data into training and validation sets. This will help our model to train on the training dataset & evaluate the performance simultaneously on the validation dataset.</p>","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(train, y, test_size=0.3, random_state=101)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids = test['PassengerId']\ntest.drop(cols_to_drop, axis=1, inplace=True)\ntest.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>We have to perform the normalization step for our final test dataset also. </p>","metadata":{}},{"cell_type":"code","source":"transformer = Normalizer().fit(test) \ntransformer\ntest=transformer.transform(test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='nn'></a>\n# Neural Network Architecture (Model)","metadata":{}},{"cell_type":"markdown","source":"<center><img src='https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/Comp-1.gif' width=600 height=600></img></center>","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Now comes the major part for which this notebook is about.</p>\n\n<p style='font-size: 17px'>We'll use Keras with Tensorflow as backend to create the neural network model & perform the training.</p> <br/>\n\n\n<div class=\"alert alert-block alert-info\">\n<p style='font-size: 17px'>Before the model, let's define some terms:</p>\n\n<p style='font-size: 17px'><strong>Sequential</strong> : Sequential class groups a linear stack of layers to form a model on which training & inference can be done.</p>\n\n<p style='font-size: 17px'><strong>Dense</strong>: Dense is a densely-connected (look at the figure) neural network layer.</p>\n\n<p style='font-size: 17px'><strong>activation</strong>: The type of function to perform mathematical operations on the layers. We'll be using **sigmoid** activation function to yield the value between 0 and 1.</p>\n\n<p style='font-size: 17px'><strong>input_dims</strong>: The shape of the values that are being passed.</p>\n\n<p style='font-size: 17px'><strong>kernel_initializer</strong>: Initializers define the way to set the initial random weights of Keras layers.</p>\n\n<p style='font-size: 17px'><strong>units</strong>: It is a positive integer & is used to define the dimensionality of the output space. </p>\n\n</div>\n\n<center><figure>\n  <img src='https://stackabuse.s3.amazonaws.com/media/deep-learning-in-keras-building-a-deep-learning-model-1.png' height=600 width=600>\n  <figcaption>Basic Neural Network structure</figcaption>\n</figure></center>","metadata":{}},{"cell_type":"code","source":"model = Sequential([\n    Dense(units=32, input_dim=16, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=32, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=5, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=1, kernel_initializer = 'uniform', activation='sigmoid')\n    ])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>We can print out how our model looks or in short, it's summary, by running the following code:</p>","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='tuning'></a>\n# Model Training (with hyperparameter tuning)","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Now, at this point, we can directly run our model & make predictions. But just as we can tune our traditional sklearn models, we can also tune the neural network hyperparameters for a better prediction score.</p>\n\n# Keras Tuner\n\n<center><img src='https://i.ytimg.com/vi/s3kH7_6xF-4/maxresdefault.jpg' height=700 width=700></img></center><br/>\n\n<p style='font-size: 17px'>In order to make the neural networks choose their best parameters themselves, we can use the library <strong>keras_tuner</strong> offered by keras itself.</p>\n\n<p class=\"alert alert-block alert-info\" style='font-size: 17px'> ðŸ“Œ <strong>KerasTuner</strong> is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search.</p> \n\n<p style='font-size: 17px'>For this, we define a function and use the <code>hp</code> argument to define the hyperparameters during model creation.</p>","metadata":{}},{"cell_type":"code","source":"def build_model(hp):\n    lrate = hp.Float('lrate', 1e-4, 1e-1, sampling='log')\n    \n    model = Sequential([\n    Dense(units=32, input_dim=16, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=32, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=5, kernel_initializer = 'uniform', activation='relu'),\n    Dense(units=1, kernel_initializer = 'uniform', activation='sigmoid')\n    ])\n    \n    model.compile(optimizer=Adam(learning_rate=lrate), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Then, we need to choose a tuner class. It can be any of these: <strong>RandomSearch</strong>, <strong>BayesianOptimization</strong> and <strong>Hyperband</strong>. </p>\n\n<p style='font-size: 17px'>We'll be going with the <strong>BayesianOptimization</strong>.</p>\n\n<p style='font-size: 17px'>Then, we declare the model function, objective function and maximum trials. Here, setting <code>overwrite=True</code> will start a new search and ignore any previous results.</p>","metadata":{}},{"cell_type":"code","source":"tuner = kt.BayesianOptimization(\n    build_model,\n    objective=kt.Objective('val_accuracy', 'max'),\n    max_trials=10,\n    num_initial_points=2,\n    overwrite=False\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>And then we search through that hyperparameters space. The syntax is quite similar to the tensorflow-keras <code>predict</code> function.</p>","metadata":{}},{"cell_type":"code","source":"tuner.search(\n    x_train, y_train, epochs=5, validation_data=(x_test, y_test)\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Now, our tuner has searched through our own defined hyperparameter space. To get the best model, we just run the <code>get_best_model[0]</code> to get the model having the best objective function result.</p>","metadata":{}},{"cell_type":"code","source":"best_model = tuner.get_best_models()[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>And then, we perform the <code>fit</code> on that best model and validate on our validation set for our final training part.</p>","metadata":{}},{"cell_type":"code","source":"hist = best_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=10, batch_size=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='viz'></a>\n# Visualizing evaluation","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Now, in order to visualize our result, we'll perform the prediction on the validation dataset to get the idea about our model performance.</p>","metadata":{}},{"cell_type":"code","source":"pred = best_model.predict(x_test, batch_size=1, verbose=0)\npred","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>We see that our predictions are there but they are continuous values and not in the binary form that we want. That's because we have <code>sigmoid</code> as our activation function in our last layer. It yields the value between 0 & 1.</p>\n\n<p style='font-size: 17px'>So, in order to make it in binary form, we can define a point above which that value will be made 1 and below which it will be made 0.</p>\n\n<p style='font-size: 17px'>Here, we take that point as 0.5.</p>","metadata":{}},{"cell_type":"code","source":"y_pred = [1 if x > 0.5 else 0 for x in pred]\ny_pred","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='accuracy'></a>\n## Plotting Accuracy & Loss Plots","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=[18, 8])\nax[0].plot([None] + hist.history['accuracy'], 'o-')\nax[0].plot([None] + hist.history['val_accuracy'], 'x-')\n# Plot legend and use the best location automatically: loc = 0.\nax[0].legend(['Train acc', 'Validation acc'], loc = 0)\nax[0].set_title('Training/Validation acc per Epoch')\nax[0].set_xlabel('Epoch')\nax[0].set_ylabel('acc')\n\nax[1].plot([None] + hist.history['loss'], 'o-')\nax[1].plot([None] + hist.history['val_loss'], 'x-')\n\n# Plot legend and use the best location automatically: loc = 0.\nax[1].legend(['Train loss', \"Val loss\"], loc = 0)\nax[1].set_title('Training/Validation Loss per Epoch')\nax[1].set_xlabel('Epoch')\nax[1].set_ylabel('Loss')\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='confusion'></a>\n## Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known.</p>","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_true=y_test, y_pred=y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Now, in order to plot confusion matrix, sklearn now provides a very efficient way. Rather than just going over lines of matplotlib code, we can just import <strong>ConfusionMatrixDisplay</strong> from the <strong>metrics</strong> module to directly plot it.</p>","metadata":{}},{"cell_type":"code","source":"ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id='final'></a>\n# Performing final predictions","metadata":{}},{"cell_type":"markdown","source":"<p style='font-size: 17px'>Hence, we can perform the same on the test dataset to get it ready for the submission.</p>","metadata":{}},{"cell_type":"code","source":"preds = best_model.predict(test)\npreds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"survived = [1 if x > 0.5 else 0 for x in preds]\nsurvived","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(survived)):\n    survived[i] = bool(survived[i])\n\nsurvived","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subs = pd.DataFrame({'PassengerId': ids, 'Transported': survived})\nsubs.to_csv('subs.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3>If you are here and find this notebook useful, an upvote would really mean a lot to me & would keep me motivated.</h3>\n<h3>Thank You!</h3>\n\n<h4>Have a wonderful day! ðŸ˜„</h4>","metadata":{}}]}